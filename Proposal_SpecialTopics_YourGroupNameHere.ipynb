{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Special Topics Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Khushi Raghuvanshi\n",
    "- Keshav Tiwari\n",
    "- Maissa Nafisa\n",
    "- Oishani Bandopadhyay\n",
    "- Shivani Kedila"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic and outline\n",
    "\n",
    "### Main topic\n",
    "Explainable AI (XAI) focuses on making machine learning models more interpretable and transparent, especially in high-stakes applications like healthcare, finance, and law. The challenge lies in the complexity of \"black box\" models, which are powerful but difficult for humans to understand. XAI methods like LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) aim to shed light on these models by providing explanations for their predictions. We will also discuss the pros and cons of XAI techniques in ensuring trust, accountability, and fairness in automated decisions. This presentation will explore the ethical considerations of using black box models, weighing the benefits and risks of knowing or not knowing what’s inside. \n",
    "\n",
    "### Learning goal for students \n",
    "Students will be able to explain the concept of Explainable AI, compare XAI methods like SHAP and LIME, and evaluate the ethical implications of transparency in machine learning models.\n",
    "\n",
    "\n",
    "### Outline of the topic\n",
    "  - Introduction to Explainable AI (XAI)\n",
    "    - __KEYPOINT__: XAI is crucial for understanding and trusting machine learning models, particularly in critical domains.\n",
    "\n",
    "  - Black box\n",
    "    - What is the black box?\n",
    "    - Importance of transparency in black box models\n",
    "    - Is transparency always necessary?\n",
    "\n",
    "  - LIME\n",
    "     - What is LIME?\n",
    "     - LIME explains models locally by approximating them with simpler, interpretable models.\n",
    "     - Demo of Lime: we will show how it works on idetifying the works of an algorithm we've designed\n",
    "\n",
    "  - SHAP\n",
    "    - What is SHAP?\n",
    "    - SHAP is based on game theory and provides both global and local explanations.\n",
    "    - How does it work?\n",
    "\n",
    "  - SHAP vs LIME\n",
    "    - __KEYPOINT__ :Comparision of LIME and SHAP, and key differences in  both\n",
    "\n",
    "  - Ethics\n",
    "    - Discussion about Explainable AI vs black box models (more details in active learning section)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedagogy\n",
    "\n",
    "\n",
    "### Readings\n",
    "Readings students will need to consume before class. \n",
    "\n",
    "1. A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME <a name=\"xaimethods\"></a>[<sup>[1]</sup>](#xaimethods). \n",
    "\n",
    "https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304\n",
    "\n",
    "This reading covers the importance of explainable AI (XAI) methods in making complex machine learning models more transparent and interpretable. It introduces SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-Agnostic Explanations) as two widely used techniques for understanding model predictions. These methods are particularly valuable in high-stakes fields like healthcare, where trust and interpretability are essential for decision-making.\n",
    "\n",
    "The reading compares SHAP and LIME, highlighting their strengths and weaknesses. SHAP provides both local and global explanations using a game-theoretic approach but is computationally intensive. LIME, on the other hand, is faster and easier to implement but only offers local explanations and assumes a linear relationship in feature contributions. Understanding these differences is crucial for selecting the appropriate method based on the specific requirements of a machine learning task.\n",
    "\n",
    "Additionally, the reading discusses challenges associated with XAI, such as model dependency and feature collinearity, which can affect the reliability of explanations. It suggests solutions like using multiple models for comparison and employing additional methods like MIP and shapr to improve interpretability. Students should take away key considerations for effectively using XAI techniques and ensuring that explanations are both accurate and accessible to end-users.\n",
    "\n",
    "### Background literature\n",
    "\n",
    "\n",
    "2. Open the black box data-driven explanation of Black Box Decision Systems <a name=\"blackbox-motivation\"></a>[<sup>[3]</sup>](#blackbox-motivation).\n",
    "\n",
    "https://arxiv.org/abs/1806.09936 \n",
    "\n",
    "This reading provides a motivation for the role of explainability in AI. It asserts that explainabiltiy matters due to the fairness, trust and accountability it provides to ML systems. It introduces challenges of XAI in dealing with black-box models, biased training data and the complexities of deep learning. Henceforth, it provides XAI solutions to those challenges through local explanation techniques like LIME, Anchors, and LORE which focus on increasing model interpretability while preserving performance.\n",
    "\n",
    "This reading takes a bottom-up approach to the relevance of explainability, in a manner that helps a new student understand the signficance of XAI. Through its discussion of future research implementations of XAI and causal learning, unstructured data, deep learning models, and their consequent ethical implications, the paper contextualizes the field's overall picture to its readers and would be a dense resource to dive into during lecture.\n",
    "\n",
    "\n",
    "3. Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models <a name=\"xai-interaction\"></a>[<sup>[4]</sup>](#xai-interaction).\n",
    "\n",
    "https://doi.org/10.1145/2858036.2858529 \n",
    "\n",
    "This reading takes a human-computer interaction perspective on the presentation of black-box ML model decision making. It introduces Prospector, an interactive visual analytics system developed to help data scientists interpret models using partial feature dependence diagnostics and localized inspection. By plotting partial dependence plots, this system helps developers visualize how manipulating a feature impacts the model's predictions while keeping other feature values fixed. This enables finding non-linear effects and interactions between features. \n",
    "\n",
    "The significance of this system paper is that it provides a robust alternative to Explainable AI. HCI and AI run in interesting parallel worlds - what AI seeks to solve by computation, HCI seeks to solve by representation. The same principle applies here, in a tangible system that enables users to look into the black box in a way that directly contests that AI based approach. Delving into this system facing approach will not only provide students in class with a glimpse of what explaining the black box looks like, but also make them critically think the challenges, advantages, and necessities behind XAI systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Lecture description\n",
    "Include some details of what will be covered by traditional lecture.  A slide running order is ideal here, but probably unrealistic.  Include information about how you will apportion coverage between group members.\n",
    "\n",
    "### Active learning\n",
    "Include information about any discussions or exercises you will have students do.  Each active learning exercise needs 1 to 3 short paragraphs.  It should include \n",
    "- the element of the outline this will cover\n",
    "- brief description of framework you will provide to set off the discussion OR a description of the in class exercise\n",
    "- for how long maximum will this run? \n",
    "\n",
    "We will have an interactive section where we split the students into 2 large groups across the class. We will ask them to give us run a simple model on the UCI Loans dataset from the UCI Machine Learning Repository. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates\n",
    "- Week 8: 02/28/2025 \n",
    "- Week 9: 03/07/2025\n",
    "- Week 8: 02/26/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Expectations:\n",
    "* Weekly meetings after class will be scheduled to discuss progress, roadblocks, and next steps.\n",
    "* If anyone is stuck for more than 24 hours, they should reach out for help. \n",
    "* Regular updates will be shared by everyone to the group tp ensure we are always on the same page. \n",
    "* Final submissions will be reviewed at least 24 hours before the deadline to ensure quality and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/19  |  9 PM | Decide on subtopics  | Each team member will decide on a subtopic and thorougly research on that.| \n",
    "| 2/22  |  8 PM |  Have our sources ready | Look up research papers and decide on readings we will assign each student to before the lecture. | \n",
    "| 2/24  | 3 PM  | Have slides ready for the presentation | Each team member adds their slides to the deck about their subtopic.    |\n",
    "| 2/26  | 2 PM  | Have the interactive model ready | Code our interactive model and make a plan for the lecture. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"xaimethods\"></a>1.[^](#xaimethods): Salih A. et all (27 June 2024) A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME*. https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304<br> \n",
    "\n",
    "<a name=\"blackbox-motivation\"></a>3.[^](#blackbox-motivation): Pedreschi, D., Giannotti, F., Guidotti, R., Monreale, A., Pappalardo, L., Ruggieri, S., & Turini, F. (2018, June 26). Open the black box data-driven explanation of Black Box Decision Systems. arXiv.org. https://arxiv.org/abs/1806.09936 <br>\n",
    "\n",
    "<a name=\"xai-interaction\"></a>4.[^](#xai-interaction): Krause, J., Perer, A., & Ng, K. (2016). Interacting with predictions. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5686–5697. https://doi.org/10.1145/2858036.2858529 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
