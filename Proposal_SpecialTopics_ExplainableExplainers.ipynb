{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Special Topics Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Khushi Raghuvanshi\n",
    "- Keshav Tiwari\n",
    "- Maissa Nafisa\n",
    "- Oishani Bandopadhyay\n",
    "- Shivani Kedila"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic and outline\n",
    "\n",
    "### Main topic\n",
    "Explainable AI (XAI) focuses on making machine learning models more interpretable and transparent, especially in high-stakes applications like healthcare, finance, and law. The challenge lies in the complexity of \"black box\" models, which are powerful but difficult for humans to understand. XAI methods like LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) aim to shed light on these models by providing explanations for their predictions. We will also discuss the pros and cons of XAI techniques in ensuring trust, accountability, and fairness in automated decisions. This presentation will explore the ethical considerations of using black box models, weighing the benefits and risks of knowing or not knowing what’s inside. \n",
    "\n",
    "### Learning goal for students \n",
    "Students will be able to explain the concept of Explainable AI, compare XAI methods like SHAP and LIME, and evaluate the ethical implications of transparency in machine learning models.\n",
    "\n",
    "\n",
    "### Outline of the topic\n",
    " - Introduction to Explainable AI (XAI)\n",
    "    - __KEYPOINT__: XAI is crucial for understanding and trusting machine learning models, particularly in critical domains.\n",
    "\n",
    "  - Black box\n",
    "    - __KEYPOINT__ : What is the black box?\n",
    "    - Importance of transparency in black box models\n",
    "    - Is transparency always necessary?\n",
    "\n",
    "  - LIME\n",
    "     - __KEYPOINT__ : What is LIME?\n",
    "     - LIME explains models locally by approximating them with simpler, interpretable models.\n",
    "     - Demo of Lime: we will show how it works on idetifying the works of an algorithm we've designed\n",
    "\n",
    "  - SHAP\n",
    "    - __KEYPOINT__ : What is SHAP?\n",
    "    - SHAP is based on game theory and provides both global and local explanations.\n",
    "    - How does it work?\n",
    "\n",
    "  - SHAP vs LIME\n",
    "    - __KEYPOINT__ :Comparision of LIME and SHAP, and key differences in  both\n",
    "\n",
    "  - Ethics\n",
    "    - Discussion about Explainable AI vs black box models (more details in active learning section)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedagogy\n",
    "\n",
    "\n",
    "### Readings\n",
    "Readings students will need to consume before class. \n",
    "\n",
    "1. What is Explainable AI? <a name=”explainableoverview”></a>[<sup>[1]</sup>](#explainableoverview).\n",
    "\n",
    "https://www.ibm.com/think/topics/explainable-ai#:~:text=Explainable%20artificial%20intelligence%20(XAI)%20is,expected%20impact%20and%20potential%20biases\n",
    "\n",
    "\n",
    "This is a brief overview from IBM about what explainable AI is as a concept. This gives students a starting point on what to expect from the class, and familiarizes them with some of the concepts we will be diving deeper into in class.\n",
    "The reading approaches explainable AI from an industry perspective, focusing on the importance of explainable AI in terms of applications and risk mitigation. It goes into how XAI uses certain techniques to follow the ML algorithm closely and explain it. The main techniques it emphasizes are prediction accuracy (which touches on LIME, one of the techniques we’ll be focusing on in our slides), traceability, and decision understanding. It also briefly discusses the relationship and distinctions between explainable AI, interpretable AI, and responsible AI. IT also covers some of the benefits of explainable AI, that we hope students will learn more about through the Active Learning section and our conclusion, which will lean more into the ethics of explainable AI.\n",
    "The industry-focused approach of this article makes it a good practical overview for students to think about real world implementations. The article ends with 5 considerations for explainable AI to get better outcomes, and 3 industry use cases for explainable AI. \n",
    "\n",
    "\n",
    "2. A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME <a name=\"xaimethods\"></a>[<sup>[2]</sup>](#xaimethods). \n",
    "\n",
    "https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304\n",
    "\n",
    "This reading covers the importance of explainable AI (XAI) methods in making complex machine learning models more transparent and interpretable. It introduces SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-Agnostic Explanations) as two widely used techniques for understanding model predictions. These methods are particularly valuable in high-stakes fields like healthcare, where trust and interpretability are essential for decision-making.\n",
    "\n",
    "The reading compares SHAP and LIME, highlighting their strengths and weaknesses. SHAP provides both local and global explanations using a game-theoretic approach but is computationally intensive. LIME, on the other hand, is faster and easier to implement but only offers local explanations and assumes a linear relationship in feature contributions. Understanding these differences is crucial for selecting the appropriate method based on the specific requirements of a machine learning task.\n",
    "\n",
    "Additionally, the reading discusses challenges associated with XAI, such as model dependency and feature collinearity, which can affect the reliability of explanations. It suggests solutions like using multiple models for comparison and employing additional methods like MIP and shapr to improve interpretability. Students should take away key considerations for effectively using XAI techniques and ensuring that explanations are both accurate and accessible to end-users.\n",
    "\n",
    "\n",
    "### Background literature\n",
    "\n",
    "\n",
    "1. Open the black box data-driven explanation of Black Box Decision Systems <a name=\"blackbox-motivation\"></a>[<sup>[3]</sup>](#blackbox-motivation).\n",
    "\n",
    "https://arxiv.org/abs/1806.09936 \n",
    "\n",
    "This reading provides a motivation for the role of explainability in AI. It asserts that explainabiltiy matters due to the fairness, trust and accountability it provides to ML systems. It introduces challenges of XAI in dealing with black-box models, biased training data and the complexities of deep learning. Henceforth, it provides XAI solutions to those challenges through local explanation techniques like LIME, Anchors, and LORE which focus on increasing model interpretability while preserving performance.\n",
    "\n",
    "This reading takes a bottom-up approach to the relevance of explainability, in a manner that helps a new student understand the signficance of XAI. Through its discussion of future research implementations of XAI and causal learning, unstructured data, deep learning models, and their consequent ethical implications, the paper contextualizes the field's overall picture to its readers and would be a dense resource to dive into during lecture.\n",
    "\n",
    "\n",
    "2. Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models <a name=\"xai-interaction\"></a>[<sup>[4]</sup>](#xai-interaction).\n",
    "\n",
    "https://doi.org/10.1145/2858036.2858529 \n",
    "\n",
    "This reading takes a human-computer interaction perspective on the presentation of black-box ML model decision making. It introduces Prospector, an interactive visual analytics system developed to help data scientists interpret models using partial feature dependence diagnostics and localized inspection. By plotting partial dependence plots, this system helps developers visualize how manipulating a feature impacts the model's predictions while keeping other feature values fixed. This enables finding non-linear effects and interactions between features. \n",
    "\n",
    "The significance of this system paper is that it provides a robust alternative to Explainable AI. HCI and AI run in interesting parallel worlds - what AI seeks to solve by computation, HCI seeks to solve by representation. The same principle applies here, in a tangible system that enables users to look into the black box in a way that directly contests that AI based approach. Delving into this system facing approach will not only provide students in class with a glimpse of what explaining the black box looks like, but also make them critically think the challenges, advantages, and necessities behind XAI systems.\n",
    "\n",
    "\n",
    "3. On the use of explainable AI for susceptibility modeling: Examining the spatial pattern of SHAP values<a name=\"shapspatial\"></a>[<sup>[5]</sup>](#shapspatial).\n",
    "\n",
    "https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304\n",
    "\n",
    "We used this paper to get a deeper understanding of the SHAP values that are used to interpret a deep neural network model. This model is more complicated, and the focus on SHAP as a method for explaining the predictions show us exactly how an interpretive layer can help understand the accuracy of the model predictions. This is also an interesting problem focusing on hydro-morphological processes, assessing risk from naturally occuring hazards. The geomorphological data is an interesting application of a deep learning network and SHAP to evaluate and explain its prediction accuracy. The paper goes deeply into the local interpretation obtained from SHAP, and translates the SHAP values into maps that are easier to interpret visually, resulting in interesting SHAP plots.\n",
    "\n",
    "\n",
    "### Lecture description\n",
    "The traditional lecture component will cover the topics mentioned in the Outline section in more detail. We will introduce explainable AI over 1-2 slides, discuss what we mean by the black box in machine learning models in 2-3 slides, explain what LIME is and how it works in 4-5 slides including a demonstration and discussion, briefly explain SHAP in 3-4 slides, and briefly compare LIME and SHAP over 1 slide, before moving into the exercise and ending with a conclusion that discusses the ethics, pros, and cons of explainable AI. \n",
    "\n",
    "Group members will cover 3 slides each at least and everyone will speak. Every member will familiarize themselves with a specific subtopic in greater detail to answer questions about that subtopic. The members leading the interactive portions will be given additional speaking sections, and the entire team will help with the exercise by walking around the groups in the classroom and suggesting ideas.\n",
    "\n",
    "### Active learning\n",
    "\n",
    "#### Discussion\n",
    "This will cover understanding the black box section of the outline. This will help them understand how LIME works, and build an intuition for how we can make these models explainable.\n",
    "\n",
    "We will write a program for a classification algorithm using a black box model. Then, we will ask students what they think are the important features that contributed to the classifier making the predicition that it did. We will show them the dataset and the classification to demonstrate our model's working so that they can guess at the features.Then, we will use LIME to approximate the black box model and explain it. We will show students how close their guesses were to the actual features that were most important towards the classifier's prediction.\n",
    "\n",
    "This will take approximately 2-3 minutes, and we will complete the explanation as well within 5 minutes of starting the discussion.\n",
    "\n",
    "#### Exercise\n",
    "As an exercise before the conclusion section, which will go into the ethics of explainable AI, we will let students discuss their thoughts on the pros and cons of explainable AI vs black box models in real-world examples. This will push students to think about possible ethical concerns with the spread of black box models.\n",
    "\n",
    "We will divide the classroom into two, splitting it down the middle. Half the class will advocate for black box models that have gained popularity in recent years - including popular products such as ChatGPT's 4o and o-1 models - that are not open-source and do not have much information shared about them. The other half will advocate for explainability in AI, using open source examples such as Llama or BERT, which have published papers that are publicly available explaining their functionality to some extent and their performance on benchmarks. \n",
    "\n",
    "Both groups will be given 5 minutes to discuss their respective sides. Then, each group will present a couple of distinct points explaining the key takeaways from their discussion, for approximately 2 minutes. This will lead into our outline element about the ethics of explainable AI. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates\n",
    "- Week 8: 02/28/2025 \n",
    "- Week 9: 03/07/2025\n",
    "- Week 8: 02/26/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Expectations:\n",
    "* Weekly meetings after class will be scheduled to discuss progress, roadblocks, and next steps.\n",
    "* If anyone is stuck for more than 24 hours, they should reach out for help. \n",
    "* Regular updates will be shared by everyone to the group tp ensure we are always on the same page. \n",
    "* Final submissions will be reviewed at least 24 hours before the deadline to ensure quality and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/19  |  9 PM | Decide on subtopics  | Each team member will decide on a subtopic and thorougly research on that.| \n",
    "| 2/22  |  8 PM |  Have our sources ready | Look up research papers and decide on readings we will assign each student to before the lecture. | \n",
    "| 2/24  | 3 PM  | Have slides ready for the presentation | Each team member adds their slides to the deck about their subtopic.    |\n",
    "| 2/26  | 2 PM  | Have the interactive model ready | Code our interactive model and make a plan for the lecture. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"explainableoverview\"></a>1.[^](#explainableoverview): Ibm. (2025, February 13). What is explainable AI (XAI)?. IBM. https://www.ibm.com/think/topics/explainable-ai#:~:text=Explainable%20artificial%20intelligence%20(XAI)%20is,expected%20impact%20and%20potential%20biases.<br>\n",
    "\n",
    "<a name=\"xaimethods\"></a>2.[^](#xaimethods): Salih A. et all (27 June 2024) A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME*. https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304<br>\n",
    "\n",
    "<a name=\"blackbox-motivation\"></a>3.[^](#blackbox-motivation): Pedreschi, D., Giannotti, F., Guidotti, R., Monreale, A., Pappalardo, L., Ruggieri, S., & Turini, F. (2018, June 26). Open the black box data-driven explanation of Black Box Decision Systems. arXiv.org. https://arxiv.org/abs/1806.09936 <br>\n",
    "\n",
    "<a name=\"xai-interaction\"></a>4.[^](#xai-interaction): Krause, J., Perer, A., & Ng, K. (2016). Interacting with predictions. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5686–5697. https://doi.org/10.1145/2858036.2858529 <br>\n",
    "\n",
    "<a name=\"shapspatial\"></a>5.[^](#xaimethods): Nan Wang. et all (2024) On the use of explainable AI for susceptibility modeling: Examining the spatial pattern of SHAP values. Geoscience Frontiers, Volume 15, Issue 4. 101800. ISSN 1674-9871.\n",
    "https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400304<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
